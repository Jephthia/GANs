{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Embedding, Dense\n",
    "from tensorflow.keras.layers.experimental.preprocessing import StringLookup\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "from tensorflow.io.gfile import GFile\n",
    "from tensorflow.strings import unicode_split\n",
    "from tensorflow.data import Dataset\n",
    "\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataManager():    \n",
    "    def __init__(self, file_path):\n",
    "        self.file_path = file_path\n",
    "        self.ids_to_chars_layer = None\n",
    "    \n",
    "    def decode_text(self, ids):\n",
    "        if self.ids_to_chars_layer is None:\n",
    "            return None\n",
    "\n",
    "        return tf.strings.reduce_join(self.ids_to_chars_layer(ids), axis=-1)\n",
    "\n",
    "    def load_data(self):\n",
    "        data = GFile(self.file_path, 'rb').read().decode(encoding='UTF-8')\n",
    "\n",
    "        # Get a list of the unique characters in the text\n",
    "        vocab = list(sorted(set(data)))\n",
    "        vocab_size = len(vocab)\n",
    "\n",
    "        chars_to_ids = StringLookup(vocabulary=vocab)\n",
    "        self.ids_to_chars_layer = StringLookup(vocabulary=chars_to_ids.get_vocabulary(), invert=True)\n",
    "\n",
    "        # Split the entire text by character\n",
    "        chars = unicode_split(data, 'UTF-8')\n",
    "        ids_of_chars = chars_to_ids(chars)\n",
    "\n",
    "        # Group characters to form sequences (+1 since the targets are shifted by one)\n",
    "        sequences_ds = Dataset.from_tensor_slices(ids_of_chars)\n",
    "        sequences_ds = sequences_ds.batch(C.SEQUENCE_LENGTH+1)\n",
    "\n",
    "        # Batch the sequences\n",
    "        ds = sequences_ds.padded_batch(C.BATCH_SIZE)\n",
    "        ds = ds.map(self._to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "        ds = ds.shuffle(C.BUFFER_SIZE)\n",
    "        ds = ds.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "        return ds\n",
    "    \n",
    "    def _to_inputs_and_targets(self, sequences):\n",
    "        # Exclude the last character\n",
    "        inputs = sequences[:, :-1] # H e l l o -> H e l l\n",
    "        # Exclude the first character\n",
    "        targets = sequences[:, 1:] # H e l l o -> e l l o\n",
    "        return inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager():\n",
    "    def __init__(self):\n",
    "        self.model = None\n",
    "        self.dataset = None\n",
    "        self.data_manager = DataManager(C.DATA_PATH)\n",
    "        \n",
    "    def load_data(self):\n",
    "        self.dataset = self.data_manager.load_data()\n",
    "    \n",
    "    def load_model(self):\n",
    "        self.model = Sequential()\n",
    "        self.model.add(Embedding(input_dim=C.VOCAB_SIZE, output_dim=C.EMBEDDING_DIM, input_length=C.SEQUENCE_LENGTH))\n",
    "        self.model.add(SimpleRNN(128, return_sequences=True))\n",
    "        self.model.add(Dense(C.VOCAB_SIZE))\n",
    "\n",
    "        self.model.compile(\n",
    "            optimizer=Adam(lr=C.INITIAL_LR),\n",
    "            loss=SparseCategoricalCrossentropy(from_logits=True)\n",
    "        )\n",
    "        \n",
    "    def sample_text(self):\n",
    "        preds = self.model.predict(self.dataset)\n",
    "\n",
    "        print('preds shape', preds.shape)\n",
    "\n",
    "        sampled_indices = tf.random.categorical(preds[0], num_samples=1)\n",
    "        sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
    "        pred_text = self.data_manager.decode_text(sampled_indices)\n",
    "\n",
    "        print('pred', pred_text.numpy())\n",
    "        \n",
    "    def train(self):\n",
    "        if self.model is None:\n",
    "            self.load_model()\n",
    "            \n",
    "        if self.dataset is None:\n",
    "            self.load_data()\n",
    "            \n",
    "        self.model.fit(self.dataset, epochs=C.EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class C:\n",
    "    # --- DATA --- #\n",
    "    SEQUENCE_LENGTH = 100\n",
    "    BATCH_SIZE = 8\n",
    "    VOCAB_SIZE = 67\n",
    "    EMBEDDING_DIM = 56\n",
    "    BUFFER_SIZE = 10000\n",
    "    DATA_PATH = 'datasets/simpleline.svg'\n",
    "    \n",
    "    # -- TRAINING -- #\n",
    "    EPOCHS = 100\n",
    "    INITIAL_LR = 1e-02\n",
    "\n",
    "tm = TrainManager()\n",
    "tm.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
