{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow.experimental.numpy as tnp\n",
    "import pathlib\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "from tensorflow.data import Dataset\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.utils import normalize\n",
    "from tensorflow.keras.models import Sequential, model_from_json\n",
    "from tensorflow.keras.layers import Conv2D, Layer, Conv2DTranspose, Flatten, Dense, Reshape, LeakyReLU, Dropout\n",
    "from tensorflow.keras.layers import BatchNormalization, Lambda, Input, Concatenate\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import Callback, TensorBoard, ModelCheckpoint\n",
    "from tensorflow.keras.metrics import BinaryAccuracy, Mean\n",
    "from tensorflow.keras.losses import BinaryCrossentropy, Reduction\n",
    "from tensorflow.distribute.cluster_resolver import TPUClusterResolver\n",
    "from tensorflow.distribute import TPUStrategy\n",
    "from tensorflow.io.gfile import GFile\n",
    "\n",
    "tf.get_logger().setLevel('ERROR')\n",
    "tf.config.list_physical_devices('GPU')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For colab only, the current version of matplotlib (3.2.2) in colab is incompatible with this code, so upgrade\n",
    "# !pip install matplotlib==3.3.2 --upgrade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "HP = {\n",
    "    'batch_size_per_replica': 256,\n",
    "    'total_samples': 70000,\n",
    "    'noise_dim': 100,\n",
    "    'g_optimizer': lambda: Adam(lr=0.0002, beta_1=0.5),\n",
    "    'd_optimizer': lambda: Adam(lr=0.0002, beta_1=0.5),\n",
    "    'g_metrics': lambda: [BinaryAccuracy(name='accuracy', threshold=0.5)],\n",
    "    'd_metrics': lambda: [BinaryAccuracy(name='accuracy', threshold=0.5)],\n",
    "    'g_loss_fn': lambda: BinaryCrossentropy(reduction=Reduction.NONE),\n",
    "    'd_loss_fn': lambda: BinaryCrossentropy(reduction=Reduction.NONE),\n",
    "    'num_images_to_log': 100,\n",
    "    'use_tpu': False\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(tm):\n",
    "    # Load data\n",
    "    (X_train, _), (X_test,_) = mnist.load_data()\n",
    "\n",
    "    # Preprocess data\n",
    "    X = tnp.vstack((X_train, X_test))\n",
    "    X = tf.cast(X, dtype=tf.float32) / 255.0\n",
    "    X = tf.reshape(X, (-1, 28, 28, 1))\n",
    "#     y = tf.ones((X.shape[0], 1))\n",
    "    y = tf.constant(0.9, shape=(X.shape[0], 1), dtype=tf.float32)\n",
    "\n",
    "    ds = Dataset.from_tensor_slices((X, y))\n",
    "    ds = ds.shuffle(buffer_size=HP['total_samples']).batch(HP['batch_size_per_replica'] * tm.strategy.num_replicas_in_sync)\n",
    "    ds = ds.repeat()\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_tpu_dataset(is_training=True):\n",
    "    import tensorflow_datasets as tfds\n",
    "\n",
    "    split = 'train' if is_training else 'test'\n",
    "    dataset, info = tfds.load(name='mnist', split=split, with_info=True,\n",
    "                            as_supervised=True, try_gcs=True)\n",
    "\n",
    "    def scale(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255.0\n",
    "\n",
    "        return image, tf.constant([1.0], dtype=tf.float32)\n",
    "\n",
    "    dataset = dataset.map(scale)\n",
    "\n",
    "    # Only shuffle and repeat the dataset in training. The advantage to have a\n",
    "    # infinite dataset for training is to avoid the potential last partial batch\n",
    "    # in each epoch, so users don't need to think about scaling the gradients\n",
    "    # based on the actual batch size.\n",
    "    if is_training:\n",
    "        dataset = dataset.shuffle(10000)\n",
    "        dataset = dataset.repeat()\n",
    "\n",
    "    dataset = dataset.batch(HP['batch_size_per_replica'])\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataset(tm):\n",
    "    if HP['use_tpu']:\n",
    "        return tm.strategy.experimental_distribute_datasets_from_function(load_tpu_dataset)\n",
    "    else:\n",
    "        return tm.strategy.experimental_distribute_dataset(load_dataset(tm))\n",
    "    \n",
    "def get_log_dir():\n",
    "    return f'gs://{os.environ[\"GS_BUCKET\"]}/mnist/logs' if HP['use_tpu'] else 'logs'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273/273 [==============================] - 57s 209ms/step - g_loss: 1.2838 - g_accuracy: 0.0000e+00 - d_loss: 0.4874 - d_accuracy: 0.4713\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f464fb970a0>"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tm = TrainManager(log_dir=get_log_dir(), experiment='test')\n",
    "\n",
    "tm.init_tpu()\n",
    "tm.init_auth()\n",
    "tm.init_strategy()\n",
    "\n",
    "ds = get_dataset(tm)\n",
    "\n",
    "steps_per_epoch = HP['total_samples'] // tm.global_batch_size\n",
    "\n",
    "with tm.strategy.scope():\n",
    "    gan, last_epoch = tm.init_gan(restore_latest=False, save_architecture=False)\n",
    "    \n",
    "callbacks = [\n",
    "#     LogGeneratedResults(num_images=HP['num_images_to_log'], log_dir=tm.generated_results_dir),\n",
    "#     TensorBoard(log_dir=tm.tensorboard_dir, update_freq='epoch'),\n",
    "#     SaveModel(log_dir=tm.model_dir)\n",
    "]\n",
    "\n",
    "epochs = 1\n",
    "gan.fit(ds, epochs=epochs+last_epoch, steps_per_epoch=steps_per_epoch, callbacks=callbacks, initial_epoch=last_epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainManager:\n",
    "    def __init__(self, log_dir, experiment='test'):\n",
    "        self.log_dir = log_dir\n",
    "        self.experiment = experiment\n",
    "        self._tpu_built = False\n",
    "        self._authenticated = False\n",
    "    \n",
    "    def init_timestamp(self, latest=False):\n",
    "        if latest:\n",
    "            all_timestamps = tf.io.gfile.listdir(self.experiment_dir)\n",
    "            if len(all_timestamps) == 0:\n",
    "                raise ValueError(f'Cannot restore latest as there are no timestamps in dir: \"{self.experiment_dir}\"')\n",
    "            timestamp = max(all_timestamps)\n",
    "        else:\n",
    "            timestamp = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "            \n",
    "        self._curr_timestamp = timestamp\n",
    "        \n",
    "    def init_gan(self, restore_latest=False, save_architecture=True):\n",
    "        if not hasattr(self, '_curr_timestamp'):\n",
    "            self.init_timestamp(restore_latest)\n",
    "\n",
    "        if restore_latest:\n",
    "            gan = GAN.load(self.model_dir, latest=True)\n",
    "            return gan, self.last_epoch\n",
    "        else:\n",
    "            gan = build_gan()\n",
    "            if save_architecture:\n",
    "                gan.save_architecture(self.model_dir)\n",
    "            last_epoch = 0\n",
    "            return gan, last_epoch\n",
    "    \n",
    "    def init_strategy(self):\n",
    "        if HP['use_tpu'] and not hasattr(self, '_resolver'):\n",
    "            self.init_tpu()\n",
    "\n",
    "        self._strategy = TPUStrategy(self._resolver) if HP['use_tpu'] else tf.distribute.get_strategy()\n",
    "    \n",
    "    def init_tpu(self):\n",
    "        # Do nothing if we already initialized or if we're not using TPUs\n",
    "        if self._tpu_built or not HP['use_tpu']:\n",
    "            return\n",
    "\n",
    "        self._resolver = TPUClusterResolver(tpu=f'grpc://{os.environ[\"COLAB_TPU_ADDR\"]}')\n",
    "        tf.config.experimental_connect_to_cluster(self._resolver)\n",
    "        tf.tpu.experimental.initialize_tpu_system(self._resolver)\n",
    "        self._tpu_built = True\n",
    "\n",
    "        tf.config.list_logical_devices()\n",
    "        \n",
    "    def init_auth(self):\n",
    "        # Do nothing if we already authenticated or if we're not using TPUs\n",
    "        if self._authenticated or not HP['use_tpu']:\n",
    "            return\n",
    "\n",
    "        if self.is_colab:\n",
    "            from google.colab import auth\n",
    "            # Authenticates the Colab machine and also the TPU using your\n",
    "            # credentials so that they can access your private GCS buckets.\n",
    "            auth.authenticate_user()\n",
    "            self._authenticated = True\n",
    "\n",
    "    @property\n",
    "    def experiment_dir(self):\n",
    "        return os.path.join(self.log_dir, self.experiment)\n",
    "        \n",
    "    @property\n",
    "    def timestamp_dir(self):\n",
    "        return os.path.join(self.experiment_dir, self._curr_timestamp)\n",
    "        \n",
    "    @property\n",
    "    def model_dir(self):\n",
    "        return os.path.join(self.timestamp_dir, 'models')\n",
    "    \n",
    "    @property\n",
    "    def tensorboard_dir(self):\n",
    "        return os.path.join(self.timestamp_dir, 'tensorboard')\n",
    "    \n",
    "    @property\n",
    "    def generated_results_dir(self):\n",
    "        return os.path.join(self.timestamp_dir, 'generated_results')\n",
    "    \n",
    "    @property\n",
    "    def last_epoch(self):\n",
    "        # Both the discrimator and generator have the same # of epochs so we can get either\n",
    "        latest_ckpt_path = tf.train.latest_checkpoint(os.path.join(self.model_dir, 'discriminator'))\n",
    "        path = pathlib.PurePath(latest_ckpt_path)\n",
    "        # Checkpoint prefixes are stored as epoch_x so we split to get the epoch number\n",
    "        epoch = path.name.split('_')[-1]\n",
    "        return int(epoch)\n",
    "    \n",
    "    @property\n",
    "    def strategy(self):\n",
    "        return self._strategy\n",
    "    \n",
    "    @property\n",
    "    def global_batch_size(self):\n",
    "        if not hasattr(self, 'strategy'):\n",
    "            raise ValueError(\"'strategy' hasn't been initialized yet.\")\n",
    "\n",
    "        return HP['batch_size_per_replica'] * self.strategy.num_replicas_in_sync\n",
    "    \n",
    "    @property\n",
    "    def is_colab(self):\n",
    "        # This is always set on Colab, the value is 0 or 1 depending on GPU presence\n",
    "        return 'COLAB_GPU' in os.environ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_gan():\n",
    "    generator = build_generator(HP['noise_dim'])\n",
    "    discriminator = build_discriminator(input_shape=(28, 28, 1))\n",
    "\n",
    "    gan = GAN(generator, discriminator, HP['noise_dim'])\n",
    "    gan.compile(\n",
    "        g_optimizer=HP['g_optimizer'](), \n",
    "        d_optimizer=HP['d_optimizer'](),\n",
    "        g_metrics=HP['g_metrics'](),\n",
    "        d_metrics=HP['d_metrics'](),\n",
    "        loss_fn=HP['g_loss_fn'](),\n",
    "        run_eagerly=True\n",
    "    )\n",
    "\n",
    "    return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(noise_dim):\n",
    "    model = Sequential()\n",
    "    \n",
    "    num_features = 128\n",
    "    \n",
    "    # 7x7\n",
    "    model.add(Dense(num_features * 7 * 7, input_dim=noise_dim))\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "    model.add(Reshape((7, 7, num_features)))\n",
    "    \n",
    "    # 14x14\n",
    "    model.add(Conv2DTranspose(num_features, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "    \n",
    "    # 28x28\n",
    "    model.add(Conv2DTranspose(num_features, (4, 4), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.02))\n",
    "    \n",
    "    model.add(Conv2D(1, (7, 7), activation='sigmoid', padding='same'))\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(input_shape):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same', input_shape=input_shape))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), strides=(2, 2), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.2))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.2))\n",
    "    \n",
    "    model.add(Flatten())\n",
    "#     model.add(MinibatchDiscrimination(1))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(Model):\n",
    "    def __init__(self, generator, discriminator, noise_dim, **kwargs):\n",
    "        super(GAN, self).__init__(**kwargs)\n",
    "        self.generator = generator\n",
    "        self.discriminator = discriminator\n",
    "        self.noise_dim = noise_dim\n",
    "\n",
    "    def compile(self, g_optimizer, d_optimizer, g_metrics, d_metrics, loss_fn, **kwargs):\n",
    "        super(GAN, self).compile(**kwargs)\n",
    "        \n",
    "        self.generator.compile(optimizer=g_optimizer, metrics=g_metrics, loss=loss_fn)\n",
    "        self.discriminator.compile(optimizer=d_optimizer, metrics=d_metrics, loss=loss_fn)\n",
    "        \n",
    "        # Save the compiled info so we can deserialize the model later\n",
    "        self.compiled_config = {\n",
    "            'g_optimizer': self.generator.optimizer,\n",
    "            'd_optimizer': self.discriminator.optimizer,\n",
    "            'g_metrics': g_metrics,\n",
    "            'd_metrics': d_metrics,\n",
    "            'loss_fn': loss_fn,\n",
    "            **kwargs\n",
    "        }\n",
    "\n",
    "#     @tf.function\n",
    "    def train_step(self, data):\n",
    "        X_real, y_real = data\n",
    "        global_batch_size = HP['batch_size_per_replica'] * self.distribute_strategy.num_replicas_in_sync\n",
    "\n",
    "        X_fake, y_fake = self.generate_fake_data(HP['batch_size_per_replica'], self.noise_dim)\n",
    "\n",
    "        # Should shuffle? so that it's not all real then all fake\n",
    "        X, y = tf.concat((X_real, X_fake), axis=0), tf.concat((y_real, y_fake), axis=0)\n",
    "    \n",
    "        # Train discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self.discriminator(X)\n",
    "            d_loss_per_replica = self.discriminator.compiled_loss(y, y_pred)\n",
    "            d_loss = tf.nn.compute_average_loss(d_loss_per_replica, global_batch_size=global_batch_size)\n",
    "        d_gradients = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.discriminator.optimizer.apply_gradients(zip(d_gradients, self.discriminator.trainable_weights))\n",
    "        \n",
    "#         print(y_pred[502:512, :], y[502:512, :])\n",
    "\n",
    "        # Update discriminator metrics\n",
    "        self.discriminator.compiled_metrics.update_state(y, y_pred)\n",
    "    \n",
    "#         y_gan = tf.ones((HP['batch_size_per_replica'], 1))\n",
    "        y_gan = tf.constant(0.9, shape=(HP['batch_size_per_replica'], 1), dtype=tf.float32)\n",
    "\n",
    "        # Train generator\n",
    "        with tf.GradientTape() as tape:\n",
    "            X_gan, _ = self.generate_fake_data(HP['batch_size_per_replica'], self.noise_dim)\n",
    "            y_pred = self.discriminator(X_gan)\n",
    "            g_loss_per_replica = self.generator.compiled_loss(y_gan, y_pred)\n",
    "            g_loss = tf.nn.compute_average_loss(g_loss_per_replica, global_batch_size=global_batch_size)\n",
    "        g_gradients = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.generator.optimizer.apply_gradients(zip(g_gradients, self.generator.trainable_weights))\n",
    "            \n",
    "        # Update generator metrics\n",
    "        self.generator.compiled_metrics.update_state(y_gan, y_pred)\n",
    "        \n",
    "        # Rename the metrics to g_ or d_\n",
    "        g_metrics = { f'g_{m.name}': m.result() for m in self.generator.metrics }\n",
    "        d_metrics = { f'd_{m.name}': m.result() for m in self.discriminator.metrics }\n",
    "\n",
    "        return { **g_metrics, **d_metrics }\n",
    "\n",
    "#     @tf.function\n",
    "    def generate_fake_data(self, batch_size, noise_dim):\n",
    "        noise_data = tf.random.normal(shape=(batch_size, noise_dim))\n",
    "        X_fake = self.generator(noise_data)\n",
    "        y_fake = tf.zeros((batch_size, 1))\n",
    "        \n",
    "        return X_fake, y_fake\n",
    "    \n",
    "    @property\n",
    "    def metrics(self):\n",
    "        # Resets these metrics after each epoch\n",
    "        return [*self.generator.metrics, *self.discriminator.metrics]\n",
    "    \n",
    "    def save_architecture(self, log_dir):\n",
    "        # If it's a local path we need to make sure it exists before writing\n",
    "        if not log_dir.startswith('gs://'):\n",
    "            pathlib.Path(log_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "        with GFile(os.path.join(log_dir, 'gan_architecture.json'), 'w') as json_file:\n",
    "            json_file.write(self.to_json())\n",
    "    \n",
    "    def save_weights(self, log_dir, prefix):\n",
    "        self.generator.save_weights(os.path.join(log_dir, 'generator', prefix))\n",
    "        self.discriminator.save_weights(os.path.join(log_dir, 'discriminator', prefix))\n",
    "        \n",
    "    @classmethod\n",
    "    def load(cls, log_dir, prefix=None, latest=False):\n",
    "        if not latest and not prefix:\n",
    "            raise ValueError('Either prefix or latest should be used.')\n",
    "            \n",
    "        with tf.keras.utils.custom_object_scope({'GAN': cls}):\n",
    "            saved_json = GFile(os.path.join(log_dir, 'gan_architecture.json'), 'r').read()\n",
    "            gan = model_from_json(saved_json)\n",
    "            \n",
    "        if latest:\n",
    "            generator_ckpt = tf.train.latest_checkpoint(os.path.join(log_dir, 'generator'))\n",
    "            discriminator_ckpt = tf.train.latest_checkpoint(os.path.join(log_dir, 'discriminator'))\n",
    "        else:\n",
    "            generator_ckpt = os.path.join(log_dir, 'generator', prefix)\n",
    "            discriminator_ckpt = os.path.join(log_dir, 'discriminator', prefix)\n",
    "\n",
    "        gan.generator.load_weights(generator_ckpt)\n",
    "        gan.discriminator.load_weights(discriminator_ckpt)\n",
    "        \n",
    "        return gan\n",
    "    \n",
    "    def get_config(self):\n",
    "        return {\n",
    "            'generator': self.generator,\n",
    "            'discriminator': self.discriminator,\n",
    "            'noise_dim': self.noise_dim,\n",
    "            'compiled_config': self.compiled_config\n",
    "        }\n",
    "    \n",
    "    @classmethod\n",
    "    def from_config(cls, config):\n",
    "        generator = model_from_json(json.dumps(config.pop('generator')))\n",
    "        discriminator = model_from_json(json.dumps(config.pop('discriminator')))\n",
    "        \n",
    "        compiled_config = config.pop('compiled_config')\n",
    "        compiled_config['g_optimizer'] = tf.keras.optimizers.deserialize(compiled_config['g_optimizer'])\n",
    "        compiled_config['d_optimizer'] = tf.keras.optimizers.deserialize(compiled_config['d_optimizer'])\n",
    "        compiled_config['g_metrics'] = [tf.keras.metrics.deserialize(c) for c in compiled_config['g_metrics']]\n",
    "        compiled_config['d_metrics'] = [tf.keras.metrics.deserialize(c) for c in compiled_config['d_metrics']]\n",
    "        compiled_config['loss_fn'] = tf.keras.losses.deserialize(compiled_config['loss_fn'])\n",
    "\n",
    "        gan = cls(generator, discriminator, **config)\n",
    "        gan.compile(**compiled_config)\n",
    "        \n",
    "        return gan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Log Generated Results Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogGeneratedResults(Callback):\n",
    "    def __init__(self, num_images, log_dir, **kwargs):\n",
    "        super(LogGeneratedResults, self).__init__(**kwargs)\n",
    "        self.num_images = num_images\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        if not hasattr(self.model, 'generate_fake_data'):\n",
    "            raise ValueError('The model should have a generate_fake_data function.')\n",
    "            \n",
    "        if not hasattr(self.model, 'noise_dim'):\n",
    "            raise ValueError('The model should have a noise_dim property.')\n",
    "\n",
    "        images, _ = self.model.generate_fake_data(self.num_images, self.model.noise_dim)\n",
    "        title = f'Total Images: {self.num_images} | Noise Dim: {self.model.noise_dim}'\n",
    "\n",
    "        self.log_images(images, title, self.log_dir, 'gray', epoch)\n",
    "\n",
    "    def log_images(self, images, title, log_dir, cmap, epoch):\n",
    "        assert len(images.shape) == 4\n",
    "\n",
    "        num_images = len(images)\n",
    "        figsize = int(np.ceil(np.sqrt(num_images)))\n",
    "\n",
    "        figure = plt.figure(figsize=(figsize, figsize))\n",
    "\n",
    "        for i in range(num_images):\n",
    "            plt.subplot(figsize, figsize, i + 1)\n",
    "            plt.axis('off')\n",
    "            plt.imshow(images[i, :, :, :], cmap=cmap)\n",
    "\n",
    "        # Save the plot to a PNG in memory.\n",
    "        buf = io.BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        # Closing the figure prevents it from being displayed directly inside\n",
    "        # the notebook.\n",
    "        plt.close(figure)\n",
    "        buf.seek(0)\n",
    "        # Convert PNG buffer to TF image\n",
    "        image = tf.image.decode_png(buf.getvalue(), channels=4)\n",
    "        # Add the batch dimension\n",
    "        image = tf.expand_dims(image, 0)\n",
    "\n",
    "        file_writer = tf.summary.create_file_writer(log_dir)\n",
    "\n",
    "        with file_writer.as_default():\n",
    "            tf.summary.image(title, image, step=epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SaveModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SaveModel(Callback):\n",
    "    def __init__(self, log_dir, **kwargs):\n",
    "        super(SaveModel, self).__init__(**kwargs)\n",
    "        self.log_dir = log_dir\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs):\n",
    "        self.model.save_weights(log_dir=self.log_dir, prefix=f'epoch_{epoch+1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Minibatch Discrimination Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MinibatchDiscrimination(Layer):\n",
    "    def __init__(self, kernel_dims, **kwargs):\n",
    "        super(MinibatchDiscrimination, self).__init__(**kwargs)\n",
    "        self.kernel_dims = kernel_dims\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.in_features = input_shape[-1]\n",
    "    \n",
    "    def call(self, X):\n",
    "        features = tf.reshape(X, (-1, self.in_features, self.kernel_dims)) # NxBxC\n",
    "\n",
    "        Mi = tf.expand_dims(features, axis=0) # 1xNxBxC\n",
    "\n",
    "        Mj = tf.expand_dims(features, axis=1) # Nx1xBxC\n",
    "\n",
    "        abs_diff = tnp.abs(Mi - Mj) # NxNxBxC\n",
    "\n",
    "        norm = tnp.sum(abs_diff, axis=3) # NxNxB\n",
    "    \n",
    "        outputs = tnp.sum(tnp.exp(-norm), axis=0) # NxB\n",
    "\n",
    "        return Concatenate(axis=1)((X, outputs)) # Nx(B+X.shape[-1])\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = super(MinibatchDiscrimination, self).get_config()\n",
    "        config.update({ 'kernel_dims': self.kernel_dims })\n",
    "        return config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
